<!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml"><head><title>Getting started testing</title><meta name="generator" content="Hovercraft! 1.0 http://regebro.github.com/hovercraft"></meta><meta name="author" content="Carl Meyer"></meta><meta name="description" content="a presentation on getting started testing, for PyCon US 2013"></meta><meta name="keywords" content="presentation, python, testing, pycon"></meta><link rel="stylesheet" href="css/hovercraft.css" media="all"></link><link rel="stylesheet" href="css/impressConsole.css" media="all"></link><link rel="stylesheet" href="css/highlight.css" media="all"></link><link rel="stylesheet" href="css/oddbird.css" media="all"></link></head><body class="impress-not-supported"><div id="impress" data-transition-duration="400"><div class="step" step="0" id="title" data-x="0" data-y="0"><h1 id="getting-started-testing">Getting started testing</h1><p><div class="vcard">
<a href="http://www.oddbird.net">
  <img src="images/logo.svg" alt="OddBird" class="logo" />
</a>
<h2 class="fn">Carl Meyer</h2>
<ul class="links">
  <li><a href="http://www.oddbird.net" class="org url">oddbird.net</a></li>
  <li><a href="https://twitter.com/carljm" rel="me">@carljm</a></li>
</ul>
</div></p></div><div class="step" step="1" id="thistalk" data-reveal="1" data-x="1600" data-y="0"><h1 id="this-talk">This talk</h1><ul><li>Why test?</li><li>How to write tests?</li><li>How to run your tests?</li><li>What kinds of tests to write?</li><li>Testing workflows.</li></ul></div><div class="step" step="2" data-reveal="1" data-x="3200" data-y="0"><h1 id="me">Me</h1><ul><li>Writing Python since 2002.</li><li>Professionally since 2007.</li><li>Writing a lot of tests since 2009.</li><li>Mostly web development.</li><li>OSS: pip, virtualenv, Django</li></ul><div class="notes"><p>A very brief story about me, Python, and testing...</p><p>I like to write tests. Even this slide deck has tests!</p><p>I mostly do web development, but I've tried to keep this talk general.</p><p>I didn't create these things, but I've done a lot of work on them.</p></div></div><div class="step" step="3" data-x="4800" data-y="0"><h1 id="let-s-make-a-thing">Let's make a thing!</h1><div class="notes"><p>A GitHub recommendation engine!</p><p>Find the projects you ought to know about...</p><p>It's been done already. Oh well.</p></div></div><div class="step" step="4" data-x="6400" data-y="0"><h1 id="gitrecs-py">gitrecs.py</h1><pre class="highlight code python"><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">watched1</span><span class="p">,</span> <span class="n">watched2</span><span class="p">):</span>
    <span class="sd">"""
    Return similarity score for two users.

    Users represented as iterables of watched repo names.

    Score is Jaccard index (intersection / union).

    """</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">repo</span> <span class="ow">in</span> <span class="n">watched1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">repo</span> <span class="ow">in</span> <span class="n">watched2</span><span class="p">:</span>
            <span class="n">intersection</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">union</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">watched1</span><span class="p">)</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">watched2</span><span class="p">)</span> <span class="o">-</span> <span class="n">intersection</span>

    <span class="k">return</span> <span class="n">intersection</span> <span class="o">/</span> <span class="n">union</span></pre><div class="notes"><p>Similarity score, 0 to 1.</p><p>Jaccard index: intersection over union.</p><p>Terrible implementation: buggy and slow. Software!</p><p>Now of course we want to make sure it works!</p></div></div><div class="step" step="5" data-x="8000" data-y="0"><h1 id="it-works">It works!</h1><pre class="highlight code python"><span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">],</span> <span class="p">[</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">])</span>
<span class="mf">0.25</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">],</span> <span class="p">[</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">])</span>
<span class="mf">0.5</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">],</span> <span class="p">[</span><span class="s">'d'</span><span class="p">])</span>
<span class="mf">0.0</span></pre><div class="notes"><p>So far, so good!</p><p>But here's a bug...</p></div></div><div class="step" step="6" data-x="9600" data-y="0"><h1 id="uh-oh">Uh oh</h1><pre class="highlight code python"><span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">],</span> <span class="p">[</span><span class="s">'b'</span><span class="p">])</span>
<span class="mf">0.3333333333333333</span></pre><div class="notes"><p>Jaccard index is a set metric, and our bad implementation doesn't handle
duplicates correctly. Should be 1/2, got 1/3.</p><p>Fortunately, Python's got an excellent built-in set data structure, so let's
rewrite to use that instead and fix this bug!</p></div></div><div class="step" step="7" data-x="11200" data-y="0"><h1 id="now-with-more-set">Now with more set</h1><pre class="highlight code python"><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">watched1</span><span class="p">,</span> <span class="n">watched2</span><span class="p">):</span>
    <span class="sd">"""
    Return similarity score for two users.

    Users represented as iterable of watched repo names.

    Score is Jaccard index (intersection / union).

    """</span>
    <span class="n">watched1</span><span class="p">,</span> <span class="n">watched2</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">watched1</span><span class="p">),</span> <span class="nb">set</span><span class="p">(</span><span class="n">watched2</span><span class="p">)</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="n">watched1</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">watched2</span><span class="p">)</span>
    <span class="n">union</span> <span class="o">=</span> <span class="n">watched1</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">watched2</span><span class="p">)</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">union</span><span class="p">)</span></pre></div><div class="step" step="8" data-x="12800" data-y="0"><h1 id="fixed">Fixed!</h1><pre class="highlight code python"><span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">],</span> <span class="p">[</span><span class="s">'b'</span><span class="p">])</span>
<span class="mf">0.5</span></pre><div class="notes"><p>Great, works!</p><p>But we totally rewrote it, better make sure we didn't break anything...</p></div></div><div class="step" step="9" data-x="14400" data-y="0"><h1 id="did-we-break-anything">Did we break anything?</h1><pre class="highlight code python"><span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">},</span> <span class="p">{</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">})</span>
<span class="mf">0.25</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">},</span> <span class="p">{</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">})</span>
<span class="mf">0.5</span>

<span class="o">&gt;&gt;&gt;</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">},</span> <span class="p">{</span><span class="s">'d'</span><span class="p">})</span>
<span class="mf">0.0</span></pre><div class="notes"><p>All good!</p></div></div><div class="step" step="10" data-reveal="1" data-x="16000" data-y="0"><h1 id="this-gets-old">This gets old.</h1><ul><li>Repetitive and boring.</li><li>Not easily reproducible.</li><li>Error-prone.</li></ul><div class="notes"><ul><li>The bigger the system, the worse this gets. (14th survey page.)</li><li>What happens with boring tasks? I skip them! Now I'll ship broken code!</li><li>If it breaks for you, hard to tell another developer how to see the
breakage.</li><li>Did I calculate all those results right? Will I do it right next time?</li></ul></div></div><div class="step" step="11" data-reveal="1" data-x="17600" data-y="0"><h1 id="we-re-software-developers">We're software developers!</h1><ul><li>Automating boring things is what we do.</li></ul><div class="notes"><p>We know how to handle boring repetitive tasks, we write software to automate
them!</p></div></div><div class="step" step="12" data-x="19200" data-y="0"><h1 id="test-gitrecs-py">test_gitrecs.py</h1><pre class="highlight code python"><span class="kn">from</span> <span class="nn">gitrecs</span> <span class="kn">import</span> <span class="n">similarity</span>

<span class="k">assert</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">},</span> <span class="p">{</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">})</span> <span class="o">==</span> <span class="mf">0.25</span>
<span class="k">assert</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">],</span> <span class="p">[</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">])</span> <span class="o">==</span> <span class="mf">0.5</span></pre><div class="notes"><p>Better! Easily repeatable tests.</p><p>Hmm, another bug.</p></div></div><div class="step" step="13" data-x="20800" data-y="0"><h1 id="a-bug">A bug!</h1><pre class="highlight code python"><span class="kn">from</span> <span class="nn">gitrecs</span> <span class="kn">import</span> <span class="n">similarity</span>

<span class="k">assert</span> <span class="n">similarity</span><span class="p">({},</span> <span class="p">{})</span> <span class="o">==</span> <span class="mf">0.0</span>
<span class="k">assert</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">},</span> <span class="p">{</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">})</span> <span class="o">==</span> <span class="mf">0.25</span>
<span class="k">assert</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">],</span> <span class="p">[</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">])</span> <span class="o">==</span> <span class="mf">0.5</span></pre><pre class="highlight ">Traceback (most recent call last):
  File "test_gitrecs.py", line 3, in &lt;module&gt;
    assert similarity({}, {}) == 0.0
  File "/home/carljm/gitrecs.py", line 14, in similarity
    return len(intersection) / len(union)
ZeroDivisionError: division by zero</pre><div class="notes"><p>We can fix the bug, but we have a problem with our tests: because the first
one failed, none of the others ran.</p><p>It'd be better if every test ran every time, pass or fail, so we could get a
more complete picture of what's broken and what isn't.</p></div></div><div class="step" step="14" data-x="22400" data-y="0"><pre class="highlight code python"><span class="k">def</span> <span class="nf">test_empty</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">({},</span> <span class="p">{})</span> <span class="o">==</span> <span class="mf">0.0</span>

<span class="k">def</span> <span class="nf">test_sets</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">},</span> <span class="p">{</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">})</span> <span class="o">==</span> <span class="mf">0.25</span>

<span class="k">def</span> <span class="nf">test_list_with_dupes</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">],</span> <span class="p">[</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">])</span> <span class="o">==</span> <span class="mf">0.5</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">test_empty</span><span class="p">,</span> <span class="n">test_quarter</span><span class="p">,</span> <span class="n">test_half</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">func</span><span class="p">()</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"{} FAILED: {}"</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">__name__</span><span class="p">,</span> <span class="n">e</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">"{} passed."</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="n">__name__</span><span class="p">))</span></pre><div class="notes"><p>Some code to run each test, catch any exceptions, and report whether the
test passed or failed.</p><p>Fortunately, we don't have to do this ourselves; there are test runners to
do this for us!</p></div><pre class="highlight ">test_empty FAILED: division by zero
test_sets passed.
test_list_with_dupes passed.</pre></div><div class="step" step="15" data-x="24000" data-y="0"><h1 id="pip-install-pytest">pip install pytest</h1><pre class="highlight code python"><span class="kn">from</span> <span class="nn">gitrecs</span> <span class="kn">import</span> <span class="n">similarity</span>

<span class="k">def</span> <span class="nf">test_empty</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">({},</span> <span class="p">{})</span> <span class="o">==</span> <span class="mf">0.0</span>

<span class="k">def</span> <span class="nf">test_sets</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">},</span> <span class="p">{</span><span class="s">'b'</span><span class="p">,</span> <span class="s">'c'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">})</span> <span class="o">==</span> <span class="mf">0.25</span>

<span class="k">def</span> <span class="nf">test_list_with_dupes</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">],</span> <span class="p">[</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">])</span> <span class="o">==</span> <span class="mf">0.5</span></pre><div class="notes"><p>One of these runners is pytest; we can install it and cut our test file down
to just the tests themselves, no test-running boilerplate at all.</p></div></div><div class="step" step="16" data-x="25600" data-y="0"><pre class="highlight ">$ py.test
=================== test session starts ===================
platform linux -- Python 3.3.0 -- pytest-2.3.4
collected 3 items

test_gitrecs.py F..

======================== FAILURES =========================
_______________________ test_empty ________________________

    def test_empty():
&gt;       assert similarity({}, {}) == 0.0

test_gitrecs.py:4:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

    def similarity(watched1, watched2):
        intersection = watched1.intersection(watched2)
        union = watched1.union(watched2)
&gt;       return len(intersection) / len(union)
E       ZeroDivisionError: division by zero

gitrecs.py:14: ZeroDivisionError
=========== 1 failed, 2 passed in 0.02 seconds ============</pre><div class="notes"><p>Run py.test - it automatically finds our tests (because they are in a file
whose name begins with "test", and each test function's name begins with
"test") and runs them, with isolation so that even if one fails, they all
run. (Can also run just one test file, or directory.)</p><p>It shows us the test file it found, shows a dot for each passed test and an
F for each failed one. And we get some nice helpful debugging output around
the failure too.</p></div></div><div class="step" step="17" data-x="27200" data-y="0"><h1 id="now-let-s-fix-that-bug">Now let's fix that bug.</h1><pre class="highlight code python"><span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">watched1</span><span class="p">,</span> <span class="n">watched2</span><span class="p">):</span>
    <span class="sd">"""
    Return similarity score for two users.

    Users represented as iterable of watched repos.

    Score is Jaccard index (intersection / union).

    """</span>
    <span class="n">watched1</span><span class="p">,</span> <span class="n">watched2</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">watched1</span><span class="p">),</span> <span class="nb">set</span><span class="p">(</span><span class="n">watched2</span><span class="p">)</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="n">watched1</span><span class="o">.</span><span class="n">intersection</span><span class="p">(</span><span class="n">watched2</span><span class="p">)</span>
    <span class="n">union</span> <span class="o">=</span> <span class="n">watched1</span><span class="o">.</span><span class="n">union</span><span class="p">(</span><span class="n">watched2</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">union</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">union</span><span class="p">)</span></pre></div><div class="step" step="18" data-x="28800" data-y="0"><h1 id="tests-pass-ship-it">Tests pass! Ship it!</h1><pre class="highlight ">$ py.test
=================== test session starts ===================
platform linux -- Python 3.3.0 -- pytest-2.3.4
collected 3 items

test_gitrecs.py ...

================ 3 passed in 0.02 seconds =================</pre><div class="notes"><p>Not only can we ship this code with some confidence that it works now, but
also some confidence that if we change the implementation in the future and
reintroduce any of these bugs, we'll catch it as soon as we run the tests.</p></div></div><div class="step" step="19" data-reveal="1" data-x="30400" data-y="0"><h1 id="test-runners">Test runners</h1><p>A brief synopsis and digression</p><ul><li>We saw <a href="http://pytest.org">py.test</a> in action: <tt>pip install pytest; py.test</tt></li><li><a href="https://nose.readthedocs.org/">Nose</a> is similar: <tt>pip install nose; nosetests</tt></li><li>Both can run simple function tests with asserts.</li><li><a href="http://docs.python.org/3.3/library/unittest.html">unittest</a> is in the standard library, similar to "xUnit" test frameworks in
various languages. Tests require a bit more boilerplate. <tt>python -m unittest
discover</tt></li><li>Others: <a href="http://twistedmatrix.com/trac/wiki/TwistedTrial">twisted.trial</a>, <a href="https://pypi.python.org/pypi/zope.testrunner">zope.testrunner</a></li></ul><div class="notes"><p>If all these choices are overwhelming, don't worry about it. They're all
fine, just pick one and run with it.</p></div></div><div class="step" step="20" data-x="32000" data-y="0"><h1 id="a-unittest-test">A unittest test</h1><pre class="highlight code python"><span class="kn">from</span> <span class="nn">unittest</span> <span class="kn">import</span> <span class="n">TestCase</span>
<span class="kn">from</span> <span class="nn">gitrecs</span> <span class="kn">import</span> <span class="n">similarity</span>

<span class="k">class</span> <span class="nc">TestSimilarity</span><span class="p">(</span><span class="n">TestCase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">test_empty</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">({},</span> <span class="p">{})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_sets</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">({</span><span class="s">'a'</span><span class="p">},</span> <span class="p">{</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">test_list_with_dupes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">score</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">([</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">],</span> <span class="p">[</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">assertEqual</span><span class="p">(</span><span class="n">score</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span></pre><div class="notes"><p>Note the use of methods on self (assertEqual and friends) rather than simple
asserts.</p></div></div><div class="step" step="21" data-reveal="1" data-x="33600" data-y="0"><h1 id="why-write-tests">Why write tests?</h1><ol><li>Tests tell you when your code is broken.</li><li>Tests help later developers understand your code.</li><li>Tests improve the design of your code.</li></ol><div class="notes"><ol><li>... as we just saw. "More fun to write tests on weekdays than fix bugs on
weekends." This is the primary reason most people write tests, and it's a
plenty good one.</li><li>A test is a form of documentation saying "this is what's important about
this code." In a well-tested codebase, reading the tests along with the
code they are testing is a great way to understand what that code is
supposed to be doing.</li><li>...if you listen to them. How? Let's look at an example.</li></ol></div></div><div class="step" step="22" data-x="35200" data-y="0"><h1 id="the-first-draft">The first draft</h1><pre class="highlight code python"><span class="k">class</span> <span class="nc">GithubUser</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">get_watched_repos</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Return this user's set of watched repos."""</span>
        <span class="c"># ... GitHub API querying happens here ...</span>

<span class="k">def</span> <span class="nf">similarity</span><span class="p">(</span><span class="n">user1</span><span class="p">,</span> <span class="n">user2</span><span class="p">):</span>
    <span class="sd">"""Return similarity score for given users."""</span>
    <span class="n">watched1</span> <span class="o">=</span> <span class="n">user1</span><span class="o">.</span><span class="n">get_watched_repos</span><span class="p">()</span>
    <span class="n">watched2</span> <span class="o">=</span> <span class="n">user2</span><span class="o">.</span><span class="n">get_watched_repos</span><span class="p">()</span>

    <span class="c"># ... same Jaccard index code ...</span></pre><div class="notes"><p>You may have been thinking, of course tests are easy to write when you're
testing nice simple functions like that similarity function.</p><p>Here's a secret: that wasn't the first version of similarity that I
wrote. The first version looked more like this.</p><p>Imagine writing tests for this similarity function.</p></div></div><div class="step" step="23" data-x="36800" data-y="0"><h1 id="harder-to-test">Harder to test</h1><pre class="highlight code python"><span class="k">class</span> <span class="nc">FakeGithubUser</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">watched</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">watched</span> <span class="o">=</span> <span class="n">watched</span>

    <span class="k">def</span> <span class="nf">get_watched_repos</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">watched</span>

<span class="k">def</span> <span class="nf">test_empty</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">(</span>
        <span class="n">FakeGithubUser</span><span class="p">({}),</span>
        <span class="n">FakeGithubUser</span><span class="p">({})</span>
        <span class="p">)</span> <span class="o">==</span> <span class="mf">0.5</span>

<span class="k">def</span> <span class="nf">test_sets</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">similarity</span><span class="p">(</span>
        <span class="n">FakeGithubUser</span><span class="p">({</span><span class="s">'a'</span><span class="p">}),</span>
        <span class="n">FakeGithubUser</span><span class="p">({</span><span class="s">'a'</span><span class="p">,</span> <span class="s">'b'</span><span class="p">})</span>
        <span class="p">)</span> <span class="o">==</span> <span class="mf">0.5</span></pre><div class="notes"><p>We take advantage of duck-typing and create a fake replacement for
GithubUser that doesn't go out and query the GitHub API, it just returns
whatever we tell it to.</p><p>This is a fine testing technique when testing code that has a collaborator
that is critical for its purpose. But when you have to do this, it should
cause you to ask yourself if it's essential to what you want to test, or if
the design of your code is making testing harder than it should be.</p><p>In this case, the collaborator is an avoidable distraction. What we really
want to test is the similarity calculation; GithubUser is irrelevant. We can
extract a similarity function that operates just on sets of repos so it
doesn't need to know anything about the GithubUser class, and then our tests
become much simpler.</p></div></div><div class="step" step="24" data-reveal="1" data-x="38400" data-y="0"><h1 id="testable-is-maintainable">Testable is maintainable</h1><ul><li>Code maintenance == managing change.</li><li>The less a function knows about the world, the more robust it is against
changes in the world ("principle of least knowledge").</li><li>The less a function knows about the world, the less of the world you
have to set up in order to test it.</li></ul><div class="notes"><p>Function (or class, or module - whatever the system under test)</p><p>In this case, similarity is harder to test if it knows about GithubUser,
because we have to set up a GithubUser (or a fake one) to feed to it for
every test. And it's also more fragile, because if the name of the
get_watched_repos method changes, it will break.</p><p>It knows more than it needs to know to do its job! By narrowing its vision
of the world, we make it both easier to test and easier to maintain.</p></div></div><div class="step" step="25" data-reveal="1" data-x="40000" data-y="0"><h1 id="unit-tests">Unit tests</h1><ul><li>Test one "unit" of code (function or method).</li><li>Isolated (maybe not 100%).</li><li>Small &amp; fast!</li><li>Focused: informative failures.</li><li>Require more refactoring with code changes.</li></ul><h1 id="integration-tests">Integration tests</h1><ul><li>Test that components talk to each other correctly.</li><li>Slower; exercise more code.</li><li>"Black box" end-to-end tests also called "system tests", "functional tests",
"acceptance tests."</li></ul><div class="notes"><p>Unit.</p><ul><li>These are the tests we've been looking at.</li><li>Unless collaborators are simple, replace with fakes.</li><li>Don't exercise very much code.</li></ul><p>Integration.</p><ul><li>Can be at various levels: testing integration of two different
methods/classes, up to end-to-end tests of the entire system.</li><li>Exercise more code; may also exercise external systems (e.g. database)
that slow tests down.</li></ul></div></div><div class="step" step="26" data-reveal="1" data-x="41600" data-y="0"><h1 id="workflows-for-testing">Workflows for testing</h1><p>Making testing a habit instead of a chore.</p><ul><li>Test first or test last?</li><li>Test-first is more fun!</li></ul><div class="notes"><p>Too much religion around this question. Tests will help you no matter when
you write them.</p><p>First you formulate clearly what you want your code to do, then you get to
write code and get the satisfaction of seeing the test pass. Psychology of
test-last is all wrong: first you get it working, then writing tests feels
like a chore.</p></div></div><div class="step" step="27" data-reveal="1" data-x="43200" data-y="0"><h1 id="a-feature-adding-workflow">A feature-adding workflow</h1><ul><li>Write a system test describing the working feature.</li><li>Start implementation from the outside in.</li><li>Programming by wish. "I wish I had a function that..." and stub it.</li><li>For each stubbed function, write unit tests describing how it should actually
work and complete the implementation to make those tests pass.</li></ul></div><div class="step" step="28" data-x="44800" data-y="0"><h1 id="programming-by-wish">Programming by wish</h1><pre class="highlight code python"><span class="k">def</span> <span class="nf">recommend_repos_for</span><span class="p">(</span><span class="n">username</span><span class="p">):</span>
    <span class="n">my_watched</span> <span class="o">=</span> <span class="n">get_watched_repos</span><span class="p">(</span><span class="n">username</span><span class="p">)</span>

    <span class="o">...</span>

<span class="k">def</span> <span class="nf">get_watched_repos</span><span class="p">(</span><span class="n">username</span><span class="p">):</span>
    <span class="c"># stub</span>
    <span class="k">return</span> <span class="p">{</span><span class="s">'pypa/pip'</span><span class="p">,</span> <span class="s">'pypa/virtualenv'</span><span class="p">}</span></pre></div><div class="step" step="29" data-reveal="1" data-x="46400" data-y="0"><h1 id="the-spike-workflow">The "spike" workflow</h1><p>When you need to write a bunch of exploratory code to figure out the problem
space.</p><ul><li>Strict TDD says delete your spikes and rewrite them test-first.</li><li>Try it; difference in second version can be illuminating!</li><li>...but I usually don't.</li></ul><div class="notes"><p>To write tests, you have to have some idea of the shape of the code you need
to write. Sometimes for an unfamiliar problem space you have to write the
code first to learn about what kind of shape it needs.</p></div></div><div class="step" step="30" data-reveal="1" data-x="48000" data-y="0"><h1 id="a-bug-fixing-workflow">A bug-fixing workflow</h1><ul><li>Write a test that fails because of the bug (a regression test).</li><li>Focus the test (don't exercise more code than needed to highlight the bug).</li><li>Fix the code so the test passes.</li><li>Ship it!</li></ul><div class="notes"><p>Always, always step one. This is often part of finding and understanding the
bug. If you haven't written a failing test, you don't have a bug identified
yet.</p></div></div><div class="step" step="31" data-reveal="1" data-x="49600" data-y="0"><h1 id="a-retrofitting-workflow">A retrofitting workflow</h1><ul><li>You have a codebase without tests. It probably isn't structured for testability.</li><li>Start with system tests verifying the features you care most about.</li><li>Even if you stop there, you still win!</li><li>The integration tests give you confidence to refactor the code as you add
unit tests.</li><li>Use code coverage as a rough progress-tracking metric.</li></ul><div class="notes"><p>definition of legacy code: code without tests.</p></div></div><div class="step" step="32" data-x="51200" data-y="0"><h1 id="measuring-code-coverage">Measuring code coverage</h1><ul><li><tt>pip install coverage</tt></li><li><tt>coverage run --branch `which py.test`</tt></li><li><tt>coverage html</tt></li></ul><div class="notes"><p><tt>--branch</tt>: record not only which lines were and weren't executed, but
also whether all branches of a conditional were taken.</p><p><tt>coverage run</tt> is the most flexible way to run coverage (can run any
python script); there are also plugins for py.test and nose that give it
more integration with the test runner.</p><p><tt>coverage html</tt> generates an HTML report.</p></div></div><div class="step" step="33" data-x="52800" data-y="0"><img src="images/coverage.png" alt="" width="800px" height=""></img><div class="notes"><p>100% coverage only tells you that every line of code in your program can run
without error. That's a good minimum baseline; it doesn't tell you whether
those lines of code are actually doing the right thing! It is a useful way
to make sure that you have at least one test checking every case that is
handled by your code (including error cases).</p></div></div><div class="step" step="34" data-x="54400" data-y="0"><h1 id="see-also">See also:</h1><ul><li><a href="http://tox.readthedocs.org">tox</a>: test your library across multiple Python versions and configurations.</li><li><a href="http://www.voidspace.org.uk/python/mock/">mock</a>: easily create fakes for testing (and monkeypatch them into place, if
needed).</li></ul><ul><li><a href="http://webtest.pythonpaste.org">WebTest</a>: request/response testing for WSGI web apps.</li><li><a href="http://seleniumhq.org/">Selenium</a>: browser automation (system tests for web apps).</li></ul><div class="notes"><p>Didn't have time to cover everything in depth, but here are a few more
testing tools you should check out:</p><ul><li>If you're releasing a Python library that other devs will use, you
probably want it to support at least two or three different versions of
Python, plus possibly different versions of dependencies as well. This can
quickly grow to a matrix of a size that's very hard to manage
manually. Tox makes it easy to run your tests across this whole matrix.</li><li>If you're writing a web app, a lot of your integration tests will be "send
a request, check the response" - WebTest is a great tool for these tests.</li></ul></div></div><div class="step" step="35" data-reveal="1" data-x="56000" data-y="0"><h1 id="coding-with-tests">Coding with tests...</h1><ul><li>Is fun and satisfying!</li><li>Reduces repetitive manual testing.</li><li>Replaces fear with confidence.</li><li>Results in better code.</li><li>Worth the effort!</li></ul><div class="notes"><ul><li>Nothing like the satisfaction of seeing those rows of dots when all your
tests are passing.</li></ul><p>It's not easy: test code is real code and requires discipline, engineering,
and investment to make it correct and maintainable. But it's worth it!</p></div></div><div class="step" step="36" id="questions" data-x="57600" data-y="0"><h1 id="questions">Questions?</h1><ul><li><a href="http://oddbird.net/start-testing-preso">oddbird.net/start-testing-preso</a></li><li><a href="http://pytest.org/">pytest.org</a></li><li><a href="http://nedbatchelder.com/code/coverage/">nedbatchelder.com/code/coverage/</a></li><li><a href="http://www.voidspace.org.uk/python/mock/">www.voidspace.org.uk/python/mock/</a></li><li><a href="http://tox.readthedocs.org">tox.readthedocs.org</a></li><li><a href="http://webtest.pythonpaste.org">webtest.pythonpaste.org</a></li><li><a href="http://seleniumhq.org">seleniumhq.org</a></li></ul><p><div class="vcard">
<a href="http://www.oddbird.net">
  <img src="images/logo.svg" alt="OddBird" class="logo" />
</a>
<h2 class="fn">Carl Meyer</h2>
<ul class="links">
  <li><a href="http://www.oddbird.net" class="org url">oddbird.net</a></li>
  <li><a href="https://twitter.com/carljm" rel="me">@carljm</a></li>
</ul>
</div></p></div></div><div id="hovercraft-help" class="hide"><table><tr><th>Space</th><td>Forward</td></tr><tr><th>Left, Down, Page Down</th><td>Next slide</td></tr><tr><th>Right, Up, Page Up</th><td>Previous slide</td></tr><tr><th>P</th><td>Open presenter console</td></tr><tr><th>H</th><td>Toggle this help</td></tr></table></div><script type="text/javascript" src="js/oddbird.js"></script><script type="text/javascript" src="js/impress.js"></script><script type="text/javascript" src="js/impressConsole.js"></script><script type="text/javascript" src="js/hovercraft.js"></script></body></html>